{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import Field, BaseModel\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from huggingface_hub import hf_hub_download\n",
    "import uuid\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # or INFO\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    force=True  # <<< MAKE SURE this is set (forces logging config to apply)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Constants ---\n",
    "mood_labels = [\n",
    "    \"adventurous\", \"comforting\", \"energizing\", \"romantic\",\n",
    "    \"cozy\", \"festive\", \"indulgent\", \"refreshing\"\n",
    "]\n",
    "REPO_ID = \"tlockhart/philly_reviews_with_mood.parquet\"\n",
    "FILE_NAME = \"philly_reviews_with_mood.parquet\"\n",
    "CACHE_PATH = \"mood_by_business_cache.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations - Enter API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 14:06:01,988 - INFO - Use pytorch device_name: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 14:06:01,989 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-04-10 14:06:01,992 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-10 14:06:02,208 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:02,299 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:03,066 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:03,170 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:03,287 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:03,411 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "2025-04-10 14:06:03,685 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:03,958 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:04,094 - DEBUG - https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/1.1\" 200 6757\n",
      "2025-04-10 14:06:04,244 - DEBUG - https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1\" 200 6757\n",
      "2025-04-10 14:06:04,720 - DEBUG - https://huggingface.co:443 \"HEAD /facebook/bart-large-mnli/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# --- Model Setup ---\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "genai.configure(api_key=\"AIzaSyBHn7xZshvAeFe5ZBLV-30OyZIbSwb9BLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Model Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model - Simply Using Gemeni to Recommend Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gemini Schema & Prompt ---\n",
    "class RestaurantDetail(BaseModel):\n",
    "    phone: str = Field(description=\"Phone\")\n",
    "    address: str = Field(description=\"Address\")\n",
    "    summary: str = Field(description=\"Summary\")\n",
    "    moods: str = Field(description=\"Moods\")\n",
    "    highlight: str = Field(description=\"Highlight\")\n",
    "\n",
    "# --- Gemini Functions ---\n",
    "def get_llm_generated_recommendation(mood):\n",
    "    prompt = f\"\"\"\n",
    "    A user is feeling {mood}. Recommend one unique, real restaurant in Philadelphia that matches this mood.\n",
    "    Avoid repeating suggestions like 'Talula's Garden'.\n",
    "    Only return the restaurant name. Do not include any extra text or commentary.\n",
    "    Session ID: {uuid.uuid4()}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "        response = model.generate_content(prompt)\n",
    "        name = response.text.strip().split(\"\\n\")[0]\n",
    "        print(\"✅ Gemini recommended restaurant:\", name)\n",
    "        return name\n",
    "    except Exception as e:\n",
    "        print(\"❌ Gemini recommendation failed:\", e)\n",
    "        return \"Vedge\"\n",
    "\n",
    "\n",
    "def fetch_restaurant_details(name):\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. Return structured restaurant info about \"{name}\" in Philadelphia.\n",
    "\n",
    "Respond ONLY in **valid JSON format** exactly like this:\n",
    "\n",
    "{{\n",
    "  \"phone\": \"(215) 555-1234\",\n",
    "  \"address\": \"123 Walnut St, Philadelphia, PA 19106\",\n",
    "  \"summary\": \"A brief description of the restaurant...\",\n",
    "  \"moods\": \"e.g. romantic, cozy, festive\",\n",
    "  \"highlight\": \"e.g. known for its rooftop bar or award-winning wine list\"\n",
    "}}\n",
    "\n",
    "Do NOT include any bullet points, explanations, commentary, or text outside the JSON.\n",
    "\"\"\".strip()\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "        response = model.generate_content(prompt)\n",
    "        raw_text = response.text.strip()\n",
    "        print(\"🪵 Gemini raw text:\\n\", raw_text)\n",
    "\n",
    "        # Strip markdown-style JSON block if present\n",
    "        if raw_text.startswith(\"```json\"):\n",
    "            raw_text = raw_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "        # Try to load JSON strictly\n",
    "        data = json.loads(raw_text)\n",
    "\n",
    "        # Basic field validation\n",
    "        required_fields = [\"phone\", \"address\", \"summary\", \"moods\", \"highlight\"]\n",
    "        if not all(field in data and isinstance(data[field], str) for field in required_fields):\n",
    "            raise ValueError(\"Missing required fields in Gemini response\")\n",
    "\n",
    "        print(\"📦 Gemini returned clean structured info.\")\n",
    "        return RestaurantDetail(**data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to get details for {name}: {e}\")\n",
    "        return RestaurantDetail(\n",
    "            phone=\"Unknown\",\n",
    "            address=\"(Generated) Philadelphia\",\n",
    "            summary=\"Recommended by Gemini for the mood\",\n",
    "            moods=\"Gemini-based recommendation\",\n",
    "            highlight=\"Gemini AI pick\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Model Set Up - Please Enter In your Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optomized Step 1 - Change from Gemeni to Zero Shot Classifier for moods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Generating mood_by_business...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring reviews: 100%|██████████| 10/10 [00:06<00:00,  1.43it/s]\n",
      "/var/folders/91/rg0gdxsx0f9chfhd7ktt567m0000gn/T/ipykernel_10678/1548275336.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(average_mood_scores)\n"
     ]
    }
   ],
   "source": [
    "# --- Load Finalized Model (zero-shot) with Caching ---\n",
    "def load_final_model_data():\n",
    "    global business_df\n",
    "    business_df = pd.read_parquet(\"Data/filtered_restaurants_businesses.parquet\")\n",
    "    review_df = pd.read_parquet(\"Data/filtered_restaurants_reviews.parquet\")\n",
    "    sample_reviews = review_df.sample(n=10, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    mood_scores = []\n",
    "    for review in tqdm(sample_reviews[\"text\"], desc=\"Scoring reviews\"):\n",
    "        try:\n",
    "            result = classifier(review, candidate_labels=mood_labels, multi_label=True)\n",
    "            score_dict = dict(zip(result[\"labels\"], result[\"scores\"]))\n",
    "        except:\n",
    "            score_dict = {}\n",
    "        mood_scores.append(score_dict)\n",
    "\n",
    "    sample_reviews[\"mood_scores\"] = mood_scores\n",
    "\n",
    "    def average_mood_scores(df):\n",
    "        mood_sums = {}\n",
    "        count = len(df)\n",
    "        for mood_dict in df[\"mood_scores\"]:\n",
    "            if isinstance(mood_dict, dict):\n",
    "                for mood, score in mood_dict.items():\n",
    "                    mood_sums[mood] = mood_sums.get(mood, 0) + score\n",
    "        return {mood: mood_sums[mood] / count for mood in mood_sums}\n",
    "\n",
    "    mood_by_business = (\n",
    "        sample_reviews.groupby(\"business_id\")\n",
    "        .apply(average_mood_scores)\n",
    "        .reset_index(name=\"aggregated_moods\")\n",
    "    )\n",
    "\n",
    "    def get_top_mood_and_score(mood_dict):\n",
    "        if isinstance(mood_dict, dict) and mood_dict:\n",
    "            top_mood = max(mood_dict.items(), key=lambda x: x[1])\n",
    "            return pd.Series({\"top_mood\": top_mood[0], \"top_mood_score\": top_mood[1]})\n",
    "        return pd.Series({\"top_mood\": None, \"top_mood_score\": None})\n",
    "\n",
    "    mood_by_business = pd.merge(\n",
    "        mood_by_business,\n",
    "        business_df[[\"business_id\", \"name\", \"stars\", \"review_count\"]],\n",
    "        on=\"business_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    mood_by_business[[\"top_mood\", \"top_mood_score\"]] = mood_by_business[\"aggregated_moods\"].apply(get_top_mood_and_score)\n",
    "    mood_by_business[\"final_score\"] = mood_by_business[\"top_mood_score\"] * mood_by_business[\"stars\"] * mood_by_business[\"review_count\"]\n",
    "    mood_by_business = mood_by_business.dropna(subset=[\"top_mood\"])\n",
    "    return mood_by_business\n",
    "\n",
    "def load_or_generate_final_model_data(overwrite_cache=False):\n",
    "    if os.path.exists(CACHE_PATH) and not overwrite_cache:\n",
    "        print(\"✅ Loading precomputed mood_by_business from cache...\")\n",
    "        return pd.read_parquet(CACHE_PATH)\n",
    "    else:\n",
    "        print(\"⏳ Generating mood_by_business...\")\n",
    "        mood_by_business = load_final_model_data()\n",
    "        mood_by_business.to_parquet(CACHE_PATH)\n",
    "        return mood_by_business\n",
    "\n",
    "# Usage (force refresh with new sample size)\n",
    "mood_by_business = load_or_generate_final_model_data(overwrite_cache=True)\n",
    "\n",
    "def get_final_model_recommendation(mood, mood_by_business):\n",
    "    top_matches = mood_by_business[mood_by_business[\"top_mood\"] == mood]\n",
    "    if top_matches.empty:\n",
    "        return None\n",
    "    return top_matches.nlargest(10, \"final_score\").sample(1).iloc[0][\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformer - Pulling In preprocessed data and loading model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optomized Step 2 - Zero Shot Classifier to the Better Performing Sentence Transformer - Please note that this model was run in preprocessing to save time and the mood outputs were saved to the dataset in hugging face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please See Code below gradio output for the accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tony's Model from Local File ---\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "REPO_ID = \"tlockhart/philly_reviews_with_mood.parquet\"\n",
    "FILE_NAME = \"philly_reviews_with_mood.parquet\"\n",
    "\n",
    "def load_parquet_from_huggingface(repo_id=REPO_ID, filename=FILE_NAME):\n",
    "    path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\")\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def recommend_restaurant_by_mood_content(df, mood, num_of_recommendations=5):\n",
    "    mood_matches = df[df[\"mood\"] == mood]\n",
    "    if mood_matches.empty:\n",
    "        return None\n",
    "    mood_expert_id = mood_matches[\"user_id\"].value_counts().idxmax()\n",
    "    mood_expert_reviews = mood_matches[mood_matches[\"user_id\"] == mood_expert_id].copy()\n",
    "    mood_expert_reviews[\"short_review\"] = mood_expert_reviews[\"review\"].apply(\n",
    "        lambda x: x[:50] + \"...\" if isinstance(x, str) and len(x) > 50 else x\n",
    "    )\n",
    "    mood_expert_reviews = mood_expert_reviews.sort_values(by=\"review_stars\", ascending=False)\n",
    "    top_recommendations = mood_expert_reviews.head(num_of_recommendations)\n",
    "    max_score = top_recommendations[\"review_stars\"].max()\n",
    "    top_scoring_restaurants = top_recommendations[top_recommendations[\"review_stars\"] == max_score]\n",
    "    final_best = top_scoring_restaurants.sample(1, random_state=random.randint(1, 9999)).iloc[0]\n",
    "    return final_best[\"business_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Business information from recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Output Formatter ---\n",
    "def lookup_business_details(name, mood):\n",
    "    if not isinstance(name, str) or name.strip() == \"\":\n",
    "        return RestaurantDetail(\n",
    "            phone=\"Unknown\", address=\"Unknown\", summary=\"No summary available\",\n",
    "            moods=mood, highlight=\"N/A\"\n",
    "        )\n",
    "\n",
    "    match = business_df[business_df[\"name\"].str.lower() == name.lower()]\n",
    "\n",
    "    if not match.empty:\n",
    "        row = match.iloc[0]\n",
    "        dynamic_highlight = f\"Rated {row['stars']} stars with {row['review_count']} reviews.\"\n",
    "        return RestaurantDetail(\n",
    "            phone=\"N/A\",\n",
    "            address=f\"{row['address']}, {row['city']}, {row['state']} {row['postal_code']}\",\n",
    "            summary=f\"{', '.join(row['categories'].split(','))} spot.\",\n",
    "            moods=mood,\n",
    "            highlight=dynamic_highlight\n",
    "        )\n",
    "    details = fetch_restaurant_details(name)\n",
    "    if details.address == \"Unknown\":\n",
    "        return RestaurantDetail(\n",
    "            phone=\"Unknown\",\n",
    "            address=\"(Generated) Philadelphia\",\n",
    "            summary=\"Recommended by Gemini for the mood\",\n",
    "            moods=\"Gemini-based recommendation\",\n",
    "            highlight=\"Gemini AI pick\"\n",
    "        )\n",
    "    return details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UI APP LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UI + App Logic ---\n",
    "\n",
    "# --- Display Formatter ---\n",
    "def format_output(name, details):\n",
    "    return f\"\"\"\n",
    "🍽️ {name}\n",
    "📍 Address: {details.address}\n",
    "📝 Summary: {details.summary}\n",
    "🎯 Moods: {details.moods}\n",
    "✨ Highlight: {details.highlight}\n",
    "    \"\"\".strip()\n",
    "\n",
    "def get_all_recommendations(mood):\n",
    "    name1 = get_final_model_recommendation(mood, mood_by_business)\n",
    "    name2 = recommend_restaurant_by_mood_content(tony_reviews, mood)\n",
    "    name3 = get_llm_generated_recommendation(mood)\n",
    "    return (\n",
    "        format_output(name1, lookup_business_details(name1, mood)),\n",
    "        format_output(name2, lookup_business_details(name2, mood)),\n",
    "        format_output(name3, lookup_business_details(name3, mood))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Gradio Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 14:06:14,702 - DEBUG - https://huggingface.co:443 \"HEAD /datasets/tlockhart/philly_reviews_with_mood.parquet/resolve/main/philly_reviews_with_mood.parquet HTTP/1.1\" 302 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loading precomputed mood_by_business from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 14:06:17,181 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2025-04-10 14:06:17,202 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-10 14:06:17,215 - DEBUG - load_verify_locations cafile='/Users/tlockhart/anaconda3/envs/venv/lib/python3.10/site-packages/certifi/cacert.pem'\n",
      "2025-04-10 14:06:17,244 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None\n",
      "2025-04-10 14:06:17,311 - DEBUG - Using selector: KqueueSelector\n",
      "2025-04-10 14:06:17,320 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2025-04-10 14:06:17,321 - DEBUG - load_verify_locations cafile='/Users/tlockhart/anaconda3/envs/venv/lib/python3.10/site-packages/certifi/cacert.pem'\n",
      "2025-04-10 14:06:17,337 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=None socket_options=None\n",
      "2025-04-10 14:06:17,338 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x3d13a5900>\n",
      "2025-04-10 14:06:17,338 - DEBUG - send_request_headers.started request=<Request [b'GET']>\n",
      "2025-04-10 14:06:17,339 - DEBUG - send_request_headers.complete\n",
      "2025-04-10 14:06:17,339 - DEBUG - send_request_body.started request=<Request [b'GET']>\n",
      "2025-04-10 14:06:17,340 - DEBUG - send_request_body.complete\n",
      "2025-04-10 14:06:17,341 - DEBUG - receive_response_headers.started request=<Request [b'GET']>\n",
      "2025-04-10 14:06:17,341 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 10 Apr 2025 18:06:17 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])\n",
      "2025-04-10 14:06:17,342 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 14:06:17,342 - DEBUG - receive_response_body.started request=<Request [b'GET']>\n",
      "2025-04-10 14:06:17,342 - DEBUG - receive_response_body.complete\n",
      "2025-04-10 14:06:17,343 - DEBUG - response_closed.started\n",
      "2025-04-10 14:06:17,343 - DEBUG - response_closed.complete\n",
      "2025-04-10 14:06:17,343 - DEBUG - close.started\n",
      "2025-04-10 14:06:17,344 - DEBUG - close.complete\n",
      "2025-04-10 14:06:17,345 - DEBUG - load_ssl_context verify=False cert=None trust_env=True http2=False\n",
      "2025-04-10 14:06:17,347 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=3 socket_options=None\n",
      "2025-04-10 14:06:17,350 - DEBUG - https://huggingface.co:443 \"HEAD /api/telemetry/gradio/initiated HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:17,354 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x3d13a6b30>\n",
      "2025-04-10 14:06:17,356 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x3d116c2e0>\n",
      "2025-04-10 14:06:17,362 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>\n",
      "2025-04-10 14:06:17,365 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x3236128c0> server_hostname='api.gradio.app' timeout=3\n",
      "2025-04-10 14:06:17,370 - DEBUG - send_request_headers.complete\n",
      "2025-04-10 14:06:17,382 - DEBUG - send_request_body.started request=<Request [b'HEAD']>\n",
      "2025-04-10 14:06:17,387 - DEBUG - send_request_body.complete\n",
      "2025-04-10 14:06:17,393 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>\n",
      "2025-04-10 14:06:17,403 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 10 Apr 2025 18:06:17 GMT'), (b'server', b'uvicorn'), (b'content-length', b'21007'), (b'content-type', b'text/html; charset=utf-8')])\n",
      "2025-04-10 14:06:17,403 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 14:06:17,404 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>\n",
      "2025-04-10 14:06:17,404 - DEBUG - receive_response_body.complete\n",
      "2025-04-10 14:06:17,404 - DEBUG - response_closed.started\n",
      "2025-04-10 14:06:17,405 - DEBUG - response_closed.complete\n",
      "2025-04-10 14:06:17,405 - DEBUG - close.started\n",
      "2025-04-10 14:06:17,406 - DEBUG - close.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 14:06:17,411 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 14:06:17,540 - DEBUG - https://huggingface.co:443 \"HEAD /api/telemetry/gradio/launched HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:17,547 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x3d116c2b0>\n",
      "2025-04-10 14:06:17,548 - DEBUG - send_request_headers.started request=<Request [b'GET']>\n",
      "2025-04-10 14:06:17,549 - DEBUG - send_request_headers.complete\n",
      "2025-04-10 14:06:17,549 - DEBUG - send_request_body.started request=<Request [b'GET']>\n",
      "2025-04-10 14:06:17,549 - DEBUG - send_request_body.complete\n",
      "2025-04-10 14:06:17,550 - DEBUG - receive_response_headers.started request=<Request [b'GET']>\n",
      "2025-04-10 14:06:17,627 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 10 Apr 2025 18:06:17 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])\n",
      "2025-04-10 14:06:17,628 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 14:06:17,629 - DEBUG - receive_response_body.started request=<Request [b'GET']>\n",
      "2025-04-10 14:06:17,630 - DEBUG - receive_response_body.complete\n",
      "2025-04-10 14:06:17,630 - DEBUG - response_closed.started\n",
      "2025-04-10 14:06:17,631 - DEBUG - response_closed.complete\n",
      "2025-04-10 14:06:17,631 - DEBUG - close.started\n",
      "2025-04-10 14:06:17,632 - DEBUG - close.complete\n",
      "2025-04-10 14:06:23,957 - DEBUG - matplotlib data path: /Users/tlockhart/anaconda3/envs/venv/lib/python3.10/site-packages/matplotlib/mpl-data\n",
      "2025-04-10 14:06:23,963 - DEBUG - CONFIGDIR=/Users/tlockhart/.matplotlib\n",
      "2025-04-10 14:06:23,996 - DEBUG - interactive is False\n",
      "2025-04-10 14:06:23,997 - DEBUG - platform is darwin\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tlockhart/anaconda3/envs/venv/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/tlockhart/anaconda3/envs/venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/tlockhart/anaconda3/envs/venv/lib/python3.10/site-packages/gradio/blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/tlockhart/anaconda3/envs/venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1663, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/tlockhart/anaconda3/envs/venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/tlockhart/anaconda3/envs/venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2364, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/tlockhart/anaconda3/envs/venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 864, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/tlockhart/anaconda3/envs/venv/lib/python3.10/site-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/91/rg0gdxsx0f9chfhd7ktt567m0000gn/T/ipykernel_10678/3934998055.py\", line 59, in handle_mood_click\n",
      "    mood = mood_from_path(evt.value)\n",
      "  File \"/var/folders/91/rg0gdxsx0f9chfhd7ktt567m0000gn/T/ipykernel_10678/3934998055.py\", line 42, in mood_from_path\n",
      "    if os.path.normpath(selection) == os.path.normpath(img_path):\n",
      "  File \"/Users/tlockhart/anaconda3/envs/venv/lib/python3.10/posixpath.py\", line 340, in normpath\n",
      "    path = os.fspath(path)\n",
      "TypeError: expected str, bytes or os.PathLike object, not dict\n"
     ]
    }
   ],
   "source": [
    "# --- Gradio UI ---\n",
    "# --- Local image paths for moods ---\n",
    "# --- UI creation ---\n",
    "# --- Local image paths for moods ---\n",
    "mood_images = {\n",
    "    \"adventurous\": \"Pictures/adventurous.png\",\n",
    "    \"comforting\":  \"Pictures/comforting.png\",\n",
    "    \"cozy\":        \"Pictures/cozy.png\",\n",
    "    \"energizing\":  \"Pictures/energizing.png\",\n",
    "    \"festive\":     \"Pictures/festive.png\",\n",
    "    \"indulgent\":   \"Pictures/indulgent.png\",\n",
    "    \"refreshing\":  \"Pictures/refreshing.png\",\n",
    "    \"romantic\":    \"Pictures/romantic.png\",\n",
    "}\n",
    "mood_items = [(img_path, mood) for mood, img_path in mood_images.items()]\n",
    "# --- Map image path back to mood ---\n",
    "# def mood_from_path(selection):\n",
    "#     logging.info(f\"SELECTION: {selection}\")\n",
    "#     if isinstance(selection, (list, tuple)):\n",
    "#         selection = selection[0]\n",
    "#     if isinstance(selection, (list, tuple)):\n",
    "#         selection = selection[0]\n",
    "\n",
    "#     for mood, img_path in mood_images.items():\n",
    "#         if os.path.normpath(selection) == os.path.normpath(img_path):\n",
    "#             return mood\n",
    "#     return \"adventurous\"\n",
    "# def mood_from_path(selection):\n",
    "#     logging.info(f\"SELECTION: {selection}\")\n",
    "#     if isinstance(selection, (list, tuple)):\n",
    "#         selection = selection[0]\n",
    "#     if isinstance(selection, (list, tuple)):\n",
    "#         selection = selection[0]\n",
    "\n",
    "#     for mood, img_path in mood_images.items():\n",
    "#         if os.path.normpath(selection) == os.path.normpath(img_path):\n",
    "#             return mood\n",
    "#     return \"adventurous\"\n",
    "def mood_from_path(selection):\n",
    "    logging.info(f\"SELECTION: {selection}\")\n",
    "    for mood, img_path in mood_images.items():\n",
    "        if os.path.normpath(selection) == os.path.normpath(img_path):\n",
    "            return mood\n",
    "    return \"adventurous\"  # Default if not found\n",
    "# --- Trigger all models ---\n",
    "# def handle_mood_click(image_path):\n",
    "#     mood = mood_from_path(image_path)\n",
    "#     print(f\"🧠 Mood selected: {mood}\")\n",
    "#     return get_all_recommendations(mood)\n",
    "# def handle_mood_click(selected_item):\n",
    "#     image_path, mood = selected_item  # unpack the tuple\n",
    "#     print(f\"🧠 Mood selected: {mood} (image: {image_path})\")\n",
    "#     return get_all_recommendations(mood)\n",
    "# def handle_mood_click(selected_item):\n",
    "#     print(f\"🖼️ Raw selected item: {selected_item}\")\n",
    "#     return \"Testing...\"\n",
    "def handle_mood_click(evt: gr.SelectData):\n",
    "    print(f\"🖼️ Raw selected item: {evt.value}\")  # <- evt.value contains the selected image path\n",
    "    mood = mood_from_path(evt.value)\n",
    "    print(f\"🧠 Mood selected: {mood}\")\n",
    "    return get_all_recommendations(mood)\n",
    "\n",
    "# --- Gradio UI creation ---\n",
    "def create_interface():\n",
    "    with gr.Blocks() as interface:\n",
    "        gr.Markdown(\"## How do you feel today?\\nClick one of the moods below to get your restaurant recommendations.\")\n",
    "\n",
    "        # --- Gallery without preview_size ---\n",
    "        gallery = gr.Gallery(\n",
    "            value=mood_items,\n",
    "            label=\"Select Your Mood\",\n",
    "            columns=4,\n",
    "            rows=2,\n",
    "            object_fit=\"cover\",\n",
    "            allow_preview=False,\n",
    "            show_label=False,\n",
    "            height=700  # This keeps layout in view\n",
    "        )\n",
    "\n",
    "        # --- Shared output boxes below ---\n",
    "        out1 = gr.Textbox(label=\"Zeroshot - bart-large-mnli\")\n",
    "        out2 = gr.Textbox(label=\"Sentence Transformer - all-MiniLM-L6-v2\")\n",
    "        out3 = gr.Textbox(label=\"Base Model - Gemeni Recommendation\")\n",
    "\n",
    "        # --- Hook image click to recommendation function ---\n",
    "        gallery.select(\n",
    "            fn=handle_mood_click,\n",
    "            # inputs=[gallery],\n",
    "            outputs=[out1, out2, out3]\n",
    "        )\n",
    "\n",
    "    return interface\n",
    "# --- Launch ---\n",
    "# --- Load Data and Launch ---\n",
    "mood_by_business = load_or_generate_final_model_data()\n",
    "tony_reviews = load_parquet_from_huggingface(REPO_ID, FILE_NAME)\n",
    "demo = create_interface()\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE OPTOMIZATION CHECK WE DID TO UTILIZE SENTENCE TRANSFORMER -- Performs better than zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 14:06:17,559 - DEBUG - https://huggingface.co:443 \"HEAD /facebook/bart-large-mnli/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Device set to use mps:0\n",
      "2025-04-10 14:06:21,015 - INFO - Use pytorch device_name: mps\n",
      "2025-04-10 14:06:21,017 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-04-10 14:06:21,122 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:21,223 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:21,327 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:21,422 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:21,532 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:21,908 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "2025-04-10 14:06:22,025 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:22,193 - DEBUG - https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-04-10 14:06:22,335 - DEBUG - https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/1.1\" 200 6757\n",
      "2025-04-10 14:06:22,449 - DEBUG - https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1\" 200 6757\n",
      "2025-04-10 14:06:24,010 - INFO - SELECTION: {'image': {'path': '/private/var/folders/91/rg0gdxsx0f9chfhd7ktt567m0000gn/T/gradio/53123e0d9a4910f135b650fdbbc2765b00ffbd7f03a6784aed68f8dffa8c647d/adventurous.png', 'url': 'http://127.0.0.1:7860/gradio_api/file=/private/var/folders/91/rg0gdxsx0f9chfhd7ktt567m0000gn/T/gradio/53123e0d9a4910f135b650fdbbc2765b00ffbd7f03a6784aed68f8dffa8c647d/adventurous.png', 'size': None, 'orig_name': 'adventurous.png', 'mime_type': 'image/png', 'is_stream': False, 'meta': {'_type': 'gradio.FileData'}}, 'caption': 'adventurous'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ Raw selected item: {'image': {'path': '/private/var/folders/91/rg0gdxsx0f9chfhd7ktt567m0000gn/T/gradio/53123e0d9a4910f135b650fdbbc2765b00ffbd7f03a6784aed68f8dffa8c647d/adventurous.png', 'url': 'http://127.0.0.1:7860/gradio_api/file=/private/var/folders/91/rg0gdxsx0f9chfhd7ktt567m0000gn/T/gradio/53123e0d9a4910f135b650fdbbc2765b00ffbd7f03a6784aed68f8dffa8c647d/adventurous.png', 'size': None, 'orig_name': 'adventurous.png', 'mime_type': 'image/png', 'is_stream': False, 'meta': {'_type': 'gradio.FileData'}}, 'caption': 'adventurous'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17a23508be74346a0d5e91f7fd57586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2d69cf94964e189eee1a7fdbd839f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "\n",
      "Review 1: The dim lighting and soft music made it perfect for a date night.\n",
      "  True Mood        : romantic\n",
      "  Zero-Shot Predict: cozy\n",
      "  Embedding Predict: romantic\n",
      "\n",
      "Review 2: After a long day, this place just felt like a warm hug.\n",
      "  True Mood        : comforting\n",
      "  Zero-Shot Predict: cozy\n",
      "  Embedding Predict: comforting\n",
      "\n",
      "Review 3: Every dish had a spicy kick—totally fired me up!\n",
      "  True Mood        : energizing\n",
      "  Zero-Shot Predict: energizing\n",
      "  Embedding Predict: festive\n",
      "\n",
      "Review 4: We wore sweaters, had hot chocolate, and watched the snowfall from inside.\n",
      "  True Mood        : cozy\n",
      "  Zero-Shot Predict: cozy\n",
      "  Embedding Predict: cozy\n",
      "\n",
      "Review 5: Twinkling lights and Christmas songs everywhere—like a holiday dream.\n",
      "  True Mood        : festive\n",
      "  Zero-Shot Predict: festive\n",
      "  Embedding Predict: festive\n",
      "\n",
      "Review 6: The desserts were over-the-top and totally worth every bite.\n",
      "  True Mood        : indulgent\n",
      "  Zero-Shot Predict: indulgent\n",
      "  Embedding Predict: indulgent\n",
      "\n",
      "Review 7: We hiked first, then found this open-air cafe with mountain views.\n",
      "  True Mood        : refreshing\n",
      "  Zero-Shot Predict: cozy\n",
      "  Embedding Predict: cozy\n",
      "\n",
      "Review 8: Live jazz, old-school cocktails, and candlelight—it felt like time travel.\n",
      "  True Mood        : romantic\n",
      "  Zero-Shot Predict: festive\n",
      "  Embedding Predict: cozy\n",
      "\n",
      "Review 9: The staff gave warm blankets and tea on a rainy night.\n",
      "  True Mood        : comforting\n",
      "  Zero-Shot Predict: cozy\n",
      "  Embedding Predict: cozy\n",
      "\n",
      "Review 10: We danced under the stars after margaritas—total vacation vibes!\n",
      "  True Mood        : energizing\n",
      "  Zero-Shot Predict: festive\n",
      "  Embedding Predict: romantic\n",
      "\n",
      "📊 Accuracy Scores\n",
      "Zero-Shot Model Accuracy    : 0.40\n",
      "Sentence Embedding Accuracy : 0.50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "\n",
    "# Sample 10 reviews and their labeled moods\n",
    "reviews = [\n",
    "    (\"The dim lighting and soft music made it perfect for a date night.\", \"romantic\"),\n",
    "    (\"After a long day, this place just felt like a warm hug.\", \"comforting\"),\n",
    "    (\"Every dish had a spicy kick—totally fired me up!\", \"energizing\"),\n",
    "    (\"We wore sweaters, had hot chocolate, and watched the snowfall from inside.\", \"cozy\"),\n",
    "    (\"Twinkling lights and Christmas songs everywhere—like a holiday dream.\", \"festive\"),\n",
    "    (\"The desserts were over-the-top and totally worth every bite.\", \"indulgent\"),\n",
    "    (\"We hiked first, then found this open-air cafe with mountain views.\", \"refreshing\"),\n",
    "    (\"Live jazz, old-school cocktails, and candlelight—it felt like time travel.\", \"romantic\"),\n",
    "    (\"The staff gave warm blankets and tea on a rainy night.\", \"comforting\"),\n",
    "    (\"We danced under the stars after margaritas—total vacation vibes!\", \"energizing\"),\n",
    "]\n",
    "\n",
    "true_moods = [label for _, label in reviews]\n",
    "texts = [text for text, _ in reviews]\n",
    "\n",
    "# Mood options\n",
    "mood_labels = [\n",
    "    \"adventurous\", \"comforting\", \"energizing\", \"romantic\",\n",
    "    \"cozy\", \"festive\", \"indulgent\", \"refreshing\"\n",
    "]\n",
    "\n",
    "# Load models\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# --- Zero-shot predictions ---\n",
    "zero_shot_preds = []\n",
    "for text in texts:\n",
    "    result = classifier(text, candidate_labels=mood_labels)\n",
    "    zero_shot_preds.append(result[\"labels\"][0])  # top prediction\n",
    "\n",
    "# --- Embedding-based predictions ---\n",
    "# Embed all review texts and mood labels\n",
    "review_embeddings = embedder.encode(texts, convert_to_tensor=True)\n",
    "mood_embeddings = embedder.encode(mood_labels, convert_to_tensor=True)\n",
    "\n",
    "embedding_preds = []\n",
    "for review_emb in review_embeddings:\n",
    "    sims = util.pytorch_cos_sim(review_emb, mood_embeddings)[0]\n",
    "    best_idx = sims.argmax().item()\n",
    "    embedding_preds.append(mood_labels[best_idx])\n",
    "\n",
    "# --- Accuracy ---\n",
    "zero_shot_acc = accuracy_score(true_moods, zero_shot_preds)\n",
    "embedding_acc = accuracy_score(true_moods, embedding_preds)\n",
    "\n",
    "# --- Print results ---\n",
    "print(\"\\nEvaluation Results:\\n\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Review {i+1}: {text}\")\n",
    "    print(f\"  True Mood        : {true_moods[i]}\")\n",
    "    print(f\"  Zero-Shot Predict: {zero_shot_preds[i]}\")\n",
    "    print(f\"  Embedding Predict: {embedding_preds[i]}\\n\")\n",
    "\n",
    "print(\"📊 Accuracy Scores\")\n",
    "print(f\"Zero-Shot Model Accuracy    : {zero_shot_acc:.2f}\")\n",
    "print(f\"Sentence Embedding Accuracy : {embedding_acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
